//! Integration tests for Zarr V2 reading.
//!
//! These tests rely on test data generated by the Python script in
//! `zarrgentest/main.py`.  Run `cd zarrgentest && uv run python main.py`
//! before running the tests if the data doesn't exist yet.

use std::path::Path;
use std::sync::Arc;

use simplezarr::array::UnifiedZarrArray;
use simplezarr::error::ZarrResult;
use simplezarr::store::LocalBackend;
use simplezarr::v2;

// ---------------------------------------------------------------------------
// Paths
// ---------------------------------------------------------------------------

const TEST_DATA_DIR: &str = "zarrgentest";

fn v2_test_dir() -> String {
    format!("{TEST_DATA_DIR}/output-datasets-v2")
}

fn var_path(dataset: &str, var: &str) -> String {
    format!("{}/{var}", dataset)
}

/// Create a `LocalBackend` rooted at the V2 output directory.
fn v2_store() -> Arc<LocalBackend> {
    Arc::new(LocalBackend::new(v2_test_dir()))
}

/// Skip (pass) if the test data directory hasn't been generated.
fn require_test_data() {
    let dir = v2_test_dir();
    if !Path::new(&dir).is_dir() {
        eprintln!("SKIPPING: test data not found at {dir}. Run the Python generator first.");
    }
}

// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------

async fn open_var(dataset: &str, var: &str) -> ZarrResult<UnifiedZarrArray> {
    let store = v2_store();
    let path = var_path(dataset, var);
    v2::open(store, &path).await
}

// ---------------------------------------------------------------------------
// Test data availability
// ---------------------------------------------------------------------------

#[tokio::test]
async fn v2_test_data_exists() {
    let dir = v2_test_dir();
    assert!(
        Path::new(&dir).is_dir(),
        "V2 test data directory not found at {dir}. Run: cd zarrgentest && uv run python main.py"
    );
}

// ---------------------------------------------------------------------------
// Individual variable open + metadata
// ---------------------------------------------------------------------------

#[tokio::test]
async fn open_2m_temperature() {
    require_test_data();
    let arr = open_var("hrrr_grid_dataset.zarr", "2m_temperature")
        .await
        .expect("open 2m_temperature");
    let md = &arr.metadata;
    assert!(!md.shape.is_empty());
    assert!(md.shape.iter().all(|&d| d > 0));
    assert_eq!(md.zarr_format, 2);
    // shape should be [3, 10, 400, 400]
    assert_eq!(md.shape, vec![3, 10, 400, 400]);
}

#[tokio::test]
async fn open_total_precipitation() {
    require_test_data();
    let arr = open_var("hrrr_grid_dataset.zarr", "total_precipitation")
        .await
        .expect("open total_precipitation");
    assert_eq!(arr.metadata.zarr_format, 2);
    assert_eq!(arr.metadata.shape, vec![3, 10, 400, 400]);
}

// ---------------------------------------------------------------------------
// Read first chunk for every variable in both datasets
// ---------------------------------------------------------------------------

async fn assert_first_chunk_readable(dataset: &str, var: &str) {
    let arr = open_var(dataset, var)
        .await
        .unwrap_or_else(|e| panic!("open {dataset}/{var}: {e}"));

    let md = &arr.metadata;
    let first_key: Vec<usize> = vec![0; md.shape.len()];

    let chunk = arr
        .get_chunk(&first_key)
        .await
        .unwrap_or_else(|e| panic!("fetch chunk {dataset}/{var}: {e}"));

    assert!(
        !chunk.is_empty(),
        "chunk for {dataset}/{var} should be non-empty"
    );
}

#[tokio::test]
async fn read_chunks_hrrr_grid_dataset() {
    require_test_data();
    for var in ["2m_temperature", "total_precipitation"] {
        assert_first_chunk_readable("hrrr_grid_dataset.zarr", var).await;
    }
}

#[tokio::test]
async fn read_chunks_hrrr_grid_dataset_constant() {
    require_test_data();
    for var in ["2m_temperature", "total_precipitation"] {
        assert_first_chunk_readable("hrrr_grid_dataset_constant.zarr", var).await;
    }
}

// ---------------------------------------------------------------------------
// Wind variables -- every compressor x endianness
// ---------------------------------------------------------------------------

fn compressors() -> Vec<&'static str> {
    vec!["zlib", "blosc", "lz4", "lz4hc", "blosclz", "snappy", "zstd"]
}

fn endiannesses() -> Vec<&'static str> {
    vec!["little", "big"]
}

#[tokio::test]
async fn read_wind_variables_hrrr_grid() {
    require_test_data();
    for comp in compressors() {
        for endian in endiannesses() {
            let var = format!("wind_{comp}_{endian}");
            assert_first_chunk_readable("hrrr_grid_dataset.zarr", &var).await;
        }
    }
}

#[tokio::test]
async fn read_wind_variables_constant() {
    require_test_data();
    for comp in compressors() {
        for endian in endiannesses() {
            let var = format!("wind_{comp}_{endian}");
            assert_first_chunk_readable("hrrr_grid_dataset_constant.zarr", &var).await;
        }
    }
}

// ---------------------------------------------------------------------------
// Data validation: constant-gradient dataset
// ---------------------------------------------------------------------------

/// Read the first chunk of a variable in the constant dataset and return it
/// as a Vec<f64>.
async fn first_chunk_f64(var: &str) -> Vec<f64> {
    let arr = open_var("hrrr_grid_dataset_constant.zarr", var)
        .await
        .unwrap_or_else(|e| panic!("open constant/{var}: {e}"));

    let first_key: Vec<usize> = vec![0; arr.metadata.shape.len()];
    let chunk = arr
        .get_chunk(&first_key)
        .await
        .unwrap_or_else(|e| panic!("fetch chunk constant/{var}: {e}"));

    chunk
        .to_f64_vec()
        .unwrap_or_else(|e| panic!("to_f64 constant/{var}: {e}"))
}

#[tokio::test]
async fn validate_constant_temperature_range() {
    require_test_data();
    let data = first_chunk_f64("2m_temperature").await;
    let min = data.iter().copied().fold(f64::INFINITY, f64::min);
    let max = data.iter().copied().fold(f64::NEG_INFINITY, f64::max);
    assert!(min >= 270.0, "temp min {min} should be >= 270");
    assert!(max <= 320.0, "temp max {max} should be <= 320");
    assert!(
        (max - min) > 1.0,
        "temp range {min}..{max} should span > 1 K"
    );
}

#[tokio::test]
async fn validate_constant_precipitation_range() {
    require_test_data();
    let data = first_chunk_f64("total_precipitation").await;
    let min = data.iter().copied().fold(f64::INFINITY, f64::min);
    let max = data.iter().copied().fold(f64::NEG_INFINITY, f64::max);
    assert!(min >= 0.0, "precip min {min} should be >= 0");
    assert!(max <= 0.02, "precip max {max} should be <= 0.02");
}

#[tokio::test]
async fn validate_constant_wind_range() {
    require_test_data();
    for comp in compressors() {
        for endian in endiannesses() {
            let var = format!("wind_{comp}_{endian}");
            let data = first_chunk_f64(&var).await;
            let min = data.iter().copied().fold(f64::INFINITY, f64::min);
            let max = data.iter().copied().fold(f64::NEG_INFINITY, f64::max);
            assert!(min >= 1.0, "{var}: min {min} should be >= 1.0");
            assert!(max <= 20.0, "{var}: max {max} should be <= 20.0");
        }
    }
}

// ---------------------------------------------------------------------------
// Full array load (merge all chunks)
// ---------------------------------------------------------------------------

#[tokio::test]
async fn load_full_constant_temperature() {
    require_test_data();
    let arr = open_var("hrrr_grid_dataset_constant.zarr", "2m_temperature")
        .await
        .expect("open constant/2m_temperature");

    let data = arr.load().await.expect("load");

    // Total elements: 3 * 10 * 400 * 400 = 4_800_000
    assert_eq!(data.len(), 3 * 10 * 400 * 400);

    let min = data.iter().copied().fold(f64::INFINITY, f64::min);
    let max = data.iter().copied().fold(f64::NEG_INFINITY, f64::max);
    assert!(min >= 270.0, "full temp min {min}");
    assert!(max <= 320.0, "full temp max {max}");
}

#[tokio::test]
async fn load_full_constant_precipitation() {
    require_test_data();
    let arr = open_var("hrrr_grid_dataset_constant.zarr", "total_precipitation")
        .await
        .expect("open constant/total_precipitation");

    let data = arr.load().await.expect("load");

    assert_eq!(data.len(), 3 * 10 * 400 * 400);

    let min = data.iter().copied().fold(f64::INFINITY, f64::min);
    let max = data.iter().copied().fold(f64::NEG_INFINITY, f64::max);
    assert!(min >= 0.0, "full precip min {min}");
    assert!(max <= 0.02, "full precip max {max}");
}

#[tokio::test]
async fn load_value_preserves_type() {
    require_test_data();
    let arr = open_var("hrrr_grid_dataset_constant.zarr", "2m_temperature")
        .await
        .expect("open");

    let value = arr.load_value().await.expect("load_value");
    assert!(!value.is_empty());
}

// ---------------------------------------------------------------------------
// Group operations
// ---------------------------------------------------------------------------

#[tokio::test]
async fn open_group_consolidated() {
    require_test_data();
    let store = v2_store();
    let expected_vars: Vec<&str> = vec!["2m_temperature", "total_precipitation"];

    let group = v2::open_group(store, "hrrr_grid_dataset.zarr", &expected_vars)
        .await
        .expect("open_group consolidated");

    assert_eq!(group.metadata.zarr_format, 2);
    assert!(group.metadata.consolidated);

    for var in &expected_vars {
        assert!(
            group.arrays.contains_key(*var),
            "group should contain {var}"
        );
        let arr = &group.arrays[*var];
        assert!(!arr.metadata.shape.is_empty());
        assert!(arr.metadata.shape.iter().all(|&d| d > 0));
    }
}

#[tokio::test]
async fn open_group_not_consolidated() {
    require_test_data();
    let store = v2_store();
    let expected_vars: Vec<&str> = vec!["2m_temperature", "total_precipitation"];

    let group = v2::open_group(store, "hrrr_grid_dataset_constant.zarr", &expected_vars)
        .await
        .expect("open_group not-consolidated");

    assert_eq!(group.metadata.zarr_format, 2);
    assert!(!group.metadata.consolidated);

    for var in &expected_vars {
        assert!(
            group.arrays.contains_key(*var),
            "group should contain {var}"
        );
    }
}

#[tokio::test]
async fn group_chunk_access() {
    require_test_data();
    let store = v2_store();
    let expected_vars: Vec<&str> = vec!["2m_temperature", "total_precipitation"];

    let group = v2::open_group(store, "hrrr_grid_dataset.zarr", &expected_vars)
        .await
        .expect("open_group");

    for var in &expected_vars {
        let arr = &group.arrays[*var];
        let first_key: Vec<usize> = vec![0; arr.metadata.shape.len()];
        let chunk = arr
            .get_chunk(&first_key)
            .await
            .unwrap_or_else(|e| panic!("chunk {var}: {e}"));
        assert!(!chunk.is_empty(), "chunk for {var} should be non-empty");
    }
}

#[tokio::test]
async fn group_load_all() {
    require_test_data();
    let store = v2_store();
    let expected_vars: Vec<&str> = vec!["2m_temperature", "total_precipitation"];

    let group = v2::open_group(store, "hrrr_grid_dataset_constant.zarr", &expected_vars)
        .await
        .expect("open_group");

    let loaded = group.load_all().await.expect("load_all");

    for var in &expected_vars {
        assert!(loaded.contains_key(*var), "loaded should contain {var}");
        let data = &loaded[*var];
        assert!(!data.is_empty(), "{var} data should be non-empty");
    }
}

// ---------------------------------------------------------------------------
// Group with wind variables (consolidated, mixed compressors)
// ---------------------------------------------------------------------------

#[tokio::test]
async fn group_wind_consolidated_chunk_access() {
    require_test_data();
    let store = v2_store();

    let mut vars: Vec<String> = Vec::new();
    vars.push("2m_temperature".into());
    vars.push("total_precipitation".into());
    for comp in compressors() {
        for endian in endiannesses() {
            vars.push(format!("wind_{comp}_{endian}"));
        }
    }
    let var_refs: Vec<&str> = vars.iter().map(|s| s.as_str()).collect();

    let group = v2::open_group(store, "hrrr_grid_dataset.zarr", &var_refs)
        .await
        .expect("open_group wind consolidated");

    assert!(group.metadata.consolidated);

    for var in &var_refs {
        assert!(
            group.arrays.contains_key(*var),
            "consolidated group should contain {var}"
        );
        let arr = &group.arrays[*var];
        let first_key: Vec<usize> = vec![0; arr.metadata.shape.len()];
        let chunk = arr
            .get_chunk(&first_key)
            .await
            .unwrap_or_else(|e| panic!("chunk {var}: {e}"));
        assert!(!chunk.is_empty(), "chunk for {var} should be non-empty");
    }
}
